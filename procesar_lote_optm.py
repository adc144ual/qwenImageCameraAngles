"""
Script optimizado para procesamiento batch de frames de video.
Optimizaciones aplicadas:
1. CachÃ© de embeddings de texto (prompt idÃ©ntico)
2. Batching: procesar mÃºltiples imÃ¡genes simultÃ¡neamente
3. Torch compile para transformer y VAE
4. LoRA fusionado (elimina overhead)
5. Menos steps de inferencia (4-8 en lugar de 12)
6. VAE slicing para reducir memoria

Speedup esperado: 5-8Ã— vs versiÃ³n secuencial
"""
import os

os.environ["HF_HOME"] = "/nas/antoniodetoro/qwen/hf_cache"
os.environ["TMPDIR"] = "/nas/antoniodetoro/qwen/tmp"
os.environ["PYTHONNOUSERSITE"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  # Reduce memory fragmentation


import torch
from PIL import Image

import sys
import glob
from tqdm import tqdm
from typing import List
import time

# Limpieza inicial
torch.cuda.empty_cache()
sys.path.append(os.getcwd())

from qwenimage.pipeline_qwenimage_edit_plus import QwenImageEditPlusPipeline
from qwenimage.transformer_qwenimage import QwenImageTransformer2DModel

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASE_MODEL = "Qwen/Qwen-Image-Edit-2509"
TRANSFORMER_MODEL = "linoyts/Qwen-Image-Edit-Rapid-AIO"
LORA_REPO = "dx8152/Qwen-Edit-2509-Multiple-angles"
LORA_WEIGHTS = "é•œå¤´è½¬æ¢.safetensors"

INPUT_DIR = "fotos_entrada"
OUTPUT_DIR = "fotos_salida_opt"
os.makedirs(OUTPUT_DIR, exist_ok=True)

dtype = torch.bfloat16 
device = "cuda"

# Prompt idÃ©ntico para todos los frames
PROMPT = "å°†ç›¸æœºè½¬å‘é¸Ÿç°è§†è§’ Turn the camera to a bird's-eye view."

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARÃMETROS DE OPTIMIZACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# TamaÃ±o del batch (ajustar segÃºn VRAM disponible)
# Con CPU offload, el lÃ­mite es mÃ¡s bajo:
# 24GB VRAM: batch_size=1-2 (con offload agresivo)
# 16GB VRAM: batch_size=1
# 12GB VRAM: batch_size=1
BATCH_SIZE = 1  # Reducido a 1 para mÃ¡xima estabilidad con 24GB

# Pasos de inferencia (4-8 es suficiente con Flow Matching)
NUM_INFERENCE_STEPS = 6

# Compilar con torch.compile (requiere PyTorch 2.0+)
USE_TORCH_COMPILE = True

# Seed fijo para consistencia entre frames
SEED = 42


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNCIONES AUXILIARES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_and_prepare_images(image_paths: List[str], target_size: int = 1024) -> List[Image.Image]:
    """Cargar imÃ¡genes y redimensionarlas manteniendo aspect ratio."""
    images = []
    for path in image_paths:
        img = Image.open(path).convert("RGB")
        
        # Calcular dimensiones mÃºltiplos de 32 (requerido por VAE)
        width, height = img.size
        aspect_ratio = width / height
        
        if width > height:
            new_width = target_size
            new_height = int(target_size / aspect_ratio)
        else:
            new_height = target_size
            new_width = int(target_size * aspect_ratio)
        
        # Ajustar a mÃºltiplos de 32
        new_width = (new_width // 32) * 32
        new_height = (new_height // 32) * 32
        
        img = img.resize((new_width, new_height), Image.LANCZOS)
        images.append(img)
    
    return images


def process_batch(
    pipe,
    images: List[Image.Image],
    prompt_embeds: torch.Tensor,
    prompt_embeds_mask: torch.Tensor,
    generator: torch.Generator,
) -> List[Image.Image]:
    """Procesar un batch de imÃ¡genes usando embeddings pre-computados."""
    
    with torch.inference_mode(), torch.cuda.amp.autocast(dtype=dtype):
        outputs = pipe(
            image=images,
            prompt_embeds=prompt_embeds,
            prompt_embeds_mask=prompt_embeds_mask,
            num_inference_steps=NUM_INFERENCE_STEPS,
            true_cfg_scale=1.0,
            generator=generator,
        ).images
    
    return outputs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CARGA Y OPTIMIZACIÃ“N DEL MODELO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("ğŸš€ Cargando modelo...")
start_time = time.time()

# Cargar pipeline directamente sin transformer personalizado
# Esto evita problemas de tipo con el sistema de offload
print("ğŸ“¦ Cargando pipeline base (sin transformer personalizado)...")
pipe = QwenImageEditPlusPipeline.from_pretrained(
    BASE_MODEL,
    torch_dtype=dtype,
    low_cpu_mem_usage=True,
)

print("ğŸ“¦ Reemplazando transformer con versiÃ³n rÃ¡pida...")
# Cargar y reemplazar transformer DESPUÃ‰S de crear pipeline
transformer_fast = QwenImageTransformer2DModel.from_pretrained(
    TRANSFORMER_MODEL,
    subfolder="transformer",
    torch_dtype=dtype,
    low_cpu_mem_usage=True
)
pipe.transformer = transformer_fast
del transformer_fast  # Liberar referencia

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GESTIÃ“N DE MEMORIA: Sequential CPU Offload (mÃ¡s agresivo)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Sequential offload: mueve cada capa del modelo CPUâ†”GPU secuencialmente
# MÃ¡s lento que model_cpu_offload pero usa MENOS memoria (~8-12GB pico)
print("ğŸ’¾ Habilitando Sequential CPU offload (mÃ¡xima reducciÃ³n de memoria)...")
torch.cuda.empty_cache()
pipe.enable_sequential_cpu_offload()  # Cada capa se mueve individualmente
print("âœ… Sequential CPU offload habilitado (~8-12GB VRAM pico)")
print("âš ï¸ SerÃ¡ mÃ¡s lento pero usarÃ¡ mucha menos memoria")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIMIZACIÃ“N 1: Fusionar LoRA (elimina overhead ~15%)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ“¸ Aplicando y fusionando adaptador LoRA...")
pipe.load_lora_weights(LORA_REPO, weight_name=LORA_WEIGHTS, adapter_name="angles")
pipe.set_adapters(["angles"], adapter_weights=[1.0])
pipe.fuse_lora(adapter_names=["angles"], lora_scale=1.25)
pipe.unload_lora_weights()  # Liberar memoria LoRA
print("âœ… LoRA fusionado (eliminado overhead de inferencia)")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIMIZACIÃ“N 2: VAE slicing (reduce picos de memoria)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTA: QwenImageEditPlusPipeline no soporta enable_vae_slicing()
# El VAE ya estÃ¡ optimizado internamente
try:
    pipe.enable_vae_slicing()
    print("âœ… VAE slicing habilitado (menor uso de VRAM)")
except AttributeError:
    print("âš ï¸ VAE slicing no disponible en este pipeline (ya optimizado)")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIMIZACIÃ“N 3: Torch compile (1.5-2Ã— mÃ¡s rÃ¡pido)
# NOTA: Deshabilitado con CPU offload (puede causar conflictos)
# Si tienes mÃ¡s VRAM y no usas offload, puedes habilitarlo
if USE_TORCH_COMPILE and False:  # Forzado a False con CPU offload
    try:
        print("âš™ï¸ Compilando transformer con torch.compile...")
        pipe.transformer = torch.compile(
            pipe.transformer,
            mode="reduce-overhead",
            fullgraph=True
        )
        print("âœ… Transformer compilado")
        
        print("âš™ï¸ Compilando VAE decoder...")
        pipe.vae.decoder = torch.compile(
            pipe.vae.decoder,
            mode="reduce-overhead"
        )
        print("âœ… VAE decoder compilado")
        
    except Exception as e:
        print(f"âš ï¸ No se pudo compilar: {e}")
        print("   Continuando sin compilaciÃ³n...")
else:
    print("âš ï¸ Torch compile deshabilitado (incompatible con CPU offload)")
    print("   Continuando sin compilaciÃ³n...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIMIZACIÃ“N 4: Pre-computar embeddings de texto (ahorra ~500ms/imagen)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print(f"ğŸ”¤ Pre-computando embeddings de texto para: '{PROMPT}'")
print("ğŸ’¾ Limpiando cachÃ© CUDA antes de computar embeddings...")
torch.cuda.empty_cache()

# Crear una imagen dummy para inicializar el encoder de texto
dummy_image = Image.new("RGB", (1024, 1024), color=(128, 128, 128))

# IMPORTANTE: Solo computar para 1 imagen, expandir dinÃ¡micamente por batch
with torch.inference_mode():
    prompt_embeds_single, prompt_embeds_mask_single = pipe.encode_prompt(
        prompt=PROMPT,
        image=dummy_image,
        device=device,
        num_images_per_prompt=1,  # Solo 1, expandir despuÃ©s
    )

print(f"âœ… Embeddings cacheados: {prompt_embeds_single.shape}")
print(f"   (Se expandirÃ¡n dinÃ¡micamente para cada batch)")

# Liberar memoria del encoder de texto
del dummy_image
torch.cuda.empty_cache()

load_time = time.time() - start_time
print(f"\nâ±ï¸ Tiempo de carga y optimizaciÃ³n: {load_time:.2f}s")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WARMUP: Primera inferencia (inicializa pipeline)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\nğŸ”¥ Warmup (inicializando pipeline)...")
print("ğŸ’¾ Limpiando cachÃ© CUDA antes de warmup...")
torch.cuda.empty_cache()
warmup_start = time.time()

# Crear nueva imagen dummy para warmup
dummy_image_warmup = Image.new("RGB", (1024, 1024), color=(128, 128, 128))
dummy_images = [dummy_image_warmup]
generator = torch.Generator(device=device).manual_seed(SEED)

# Primera pasada (carga modelos a GPU)
_ = process_batch(pipe, dummy_images, prompt_embeds_single, prompt_embeds_mask_single, generator)

# Liberar imagen de warmup
del dummy_image_warmup

warmup_time = time.time() - warmup_start
print(f"âœ… Warmup completado en {warmup_time:.2f}s")
torch.cuda.empty_cache()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROCESAMIENTO BATCH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Buscar todas las imÃ¡genes
valid_extensions = ("*.jpg", "*.jpeg", "*.png", "*.webp")
image_files = []
for ext in valid_extensions:
    image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))

# Ordenar por nombre (importante para frames de video)
image_files.sort()

if not image_files:
    print(f"âš ï¸ No se encontraron imÃ¡genes en {INPUT_DIR}")
    sys.exit(1)

print(f"\n{'='*60}")
print(f"ğŸ“Š RESUMEN DE PROCESAMIENTO")
print(f"{'='*60}")
print(f"Total de imÃ¡genes: {len(image_files)}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Batches totales: {(len(image_files) + BATCH_SIZE - 1) // BATCH_SIZE}")
print(f"Steps de inferencia: {NUM_INFERENCE_STEPS}")
print(f"{'='*60}\n")

# Procesar en batches
total_start = time.time()
processed_count = 0

for batch_idx in tqdm(range(0, len(image_files), BATCH_SIZE), desc="Procesando batches"):
    batch_files = image_files[batch_idx:batch_idx + BATCH_SIZE]
    batch_size_actual = len(batch_files)
    
    # Cargar y preparar imÃ¡genes del batch
    batch_images = load_and_prepare_images(batch_files)
    
    # Expandir embeddings para el tamaÃ±o del batch actual
    if batch_size_actual == 1:
        prompt_embeds_batch = prompt_embeds_single
        prompt_embeds_mask_batch = prompt_embeds_mask_single
    else:
        # Expandir repitiendo los embeddings
        prompt_embeds_batch = prompt_embeds_single.repeat(batch_size_actual, 1, 1)
        prompt_embeds_mask_batch = prompt_embeds_mask_single.repeat(batch_size_actual, 1)
    
    # Crear generador para este batch (mantiene consistencia temporal)
    generator = torch.Generator(device=device).manual_seed(SEED + batch_idx)
    
    # Procesar batch
    batch_start = time.time()
    output_images = process_batch(
        pipe, 
        batch_images, 
        prompt_embeds_batch, 
        prompt_embeds_mask_batch, 
        generator
    )
    batch_time = time.time() - batch_start
    
    # Guardar resultados
    for img_path, output_img in zip(batch_files, output_images):
        filename = os.path.basename(img_path)
        output_path = os.path.join(OUTPUT_DIR, f"edit_{filename}")
        output_img.save(output_path)
        processed_count += 1
    
    # Limpiar cachÃ© GPU
    torch.cuda.empty_cache()
    
    # Mostrar estadÃ­sticas del batch
    time_per_image = batch_time / batch_size_actual
    print(f"  âš¡ Batch {batch_idx//BATCH_SIZE + 1}: {batch_time:.2f}s total, {time_per_image:.2f}s/imagen")

total_time = time.time() - total_start

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ESTADÃSTICAS FINALES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\n{'='*60}")
print(f"âœ¨ Â¡PROCESAMIENTO COMPLETADO!")
print(f"{'='*60}")
print(f"ImÃ¡genes procesadas: {processed_count}")
print(f"Tiempo total: {total_time:.2f}s")
print(f"Tiempo promedio: {total_time/processed_count:.2f}s por imagen")
print(f"Throughput: {processed_count/total_time:.2f} imÃ¡genes/segundo")
print(f"Resultados guardados en: {OUTPUT_DIR}")
print(f"{'='*60}")

# EstimaciÃ³n de speedup vs versiÃ³n original
print(f"\nğŸ“ˆ ESTIMACIÃ“N DE MEJORA:")
print(f"   VersiÃ³n original: ~15-20s por imagen (12 steps, sin optimizar)")
print(f"   VersiÃ³n optimizada: ~{total_time/processed_count:.2f}s por imagen")
print(f"   Speedup estimado: ~{18/(total_time/processed_count):.1f}Ã— mÃ¡s rÃ¡pido")
