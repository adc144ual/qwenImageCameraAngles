import os

os.environ["HF_HOME"] = "/nas/antoniodetoro/qwen/hf_cache"
os.environ["TMPDIR"] = "/nas/antoniodetoro/qwen/tmp"
os.environ["PYTHONNOUSERSITE"] = "1"


import torch
from PIL import Image

import sys
import glob

# 1. Limpieza inicial
torch.cuda.empty_cache()
sys.path.append(os.getcwd())

from qwenimage.pipeline_qwenimage_edit_plus import QwenImageEditPlusPipeline
from qwenimage.transformer_qwenimage import QwenImageTransformer2DModel

# ---------------- CONFIG ----------------
BASE_MODEL = "Qwen/Qwen-Image-Edit-2509"
TRANSFORMER_MODEL = "linoyts/Qwen-Image-Edit-Rapid-AIO"
LORA_REPO = "dx8152/Qwen-Edit-2509-Multiple-angles"
LORA_WEIGHTS = "ÈïúÂ§¥ËΩ¨Êç¢.safetensors"
N_STEPS = 2

INPUT_DIR = "fotos_entrada/new"
OUTPUT_DIR = "fotos_salida"
os.makedirs(OUTPUT_DIR, exist_ok=True)

dtype = torch.bfloat16 
device = "cuda"

# Prompt sugerido (Mezcla Chino/Ingl√©s para mejor activaci√≥n del LoRA)
prompt = "Â∞ÜÁõ∏Êú∫ËΩ¨ÂêëÈ∏üÁû∞ËßÜËßí Turn the camera to a bird's-eye view."

print("üöÄ Cargando componentes en modo ahorro de memoria...")

try:
    # 2. Cargar Transformer (SIN .to(device))
    transformer = QwenImageTransformer2DModel.from_pretrained(
        TRANSFORMER_MODEL,
        subfolder="transformer",
        torch_dtype=dtype,
        low_cpu_mem_usage=True
    )

    # 3. Cargar Pipeline (SIN .to(device))
    pipe = QwenImageEditPlusPipeline.from_pretrained(
        BASE_MODEL,
        transformer=transformer,
        torch_dtype=dtype,
    )

    # --- ESTRATEGIA DE MEMORIA CLAVE ---
    # En lugar de subir todo a la GPU, esto mueve m√≥dulos de RAM a VRAM solo cuando se necesitan.
    pipe.enable_sequential_cpu_offload()
    # ----------------------------------

    # 4. Cargar LoRA (Sin fusionar para evitar picos de VRAM)
    print(f"üì∏ Aplicando adaptador de √°ngulos...")
    pipe.load_lora_weights(LORA_REPO, weight_name=LORA_WEIGHTS, adapter_name="angles")
    pipe.set_adapters(["angles"], adapter_weights=[0.9])

    # 5. Bucle de procesamiento
    valid_extensions = ("*.jpg", "*.jpeg", "*.png", "*.webp")
    image_files = []
    for ext in valid_extensions:
        image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))

    if not image_files:
        print(f"‚ö†Ô∏è No se encontraron im√°genes en {INPUT_DIR}")
        sys.exit()

    print(f"‚úÖ {len(image_files)} im√°genes encontradas. Iniciando...")

    for img_path in image_files:
        filename = os.path.basename(img_path)
        print(f"üñºÔ∏è Procesando: {filename}...")

        raw_image = Image.open(img_path).convert("RGB")
        
        # Usamos un generador local para mantener consistencia
        generator = torch.Generator(device="cuda").manual_seed(42)

        with torch.inference_mode():
            output = pipe(
                image=[raw_image],
                prompt=prompt,
                num_inference_steps=N_STEPS, 
                true_cfg_scale=1.0,
                generator=generator,
            ).images[0]
            
        # Guardar y limpiar cach√© de esta iteraci√≥n
        output.save(os.path.join(OUTPUT_DIR, f"edit_{N_STEPS}_{filename}"))
        torch.cuda.empty_cache()

    print(f"\n‚ú® ¬°PROCESO COMPLETADO! Resultados en: {OUTPUT_DIR}")

except Exception as e:
    print(f"‚ùå ERROR: {e}")
    import traceback
    traceback.print_exc()